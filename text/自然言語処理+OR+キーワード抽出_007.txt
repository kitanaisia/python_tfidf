自然言語処理(NLP:Natural Language Processing)の実践的な入門書です。「自然言語」とは、英語や日本語など人々が日常のコミュニケーションで使う言語のことで、NLPに基づく技術は、モバイル端末におけるテキストの予測や手書き文字認識、検索エンジンにおける統一されていないテキスト内の情報取得、機械翻訳においてはある言語で書かれたテキストの分析と多言語への変換など、広範囲に活用されるようになってきています。本書では、NLPの理論的な基礎、理論、応用をバランスよく解説。本書の例から学び、実際のプログラムを書き、そして実装することを通して、読者はNLPを始めるための実用的な知識と技術を得られるでしょう。
関連書籍
Python クックブック 第2版
Pythonチュートリアル 第2版
集合知プログラミング
初めてのPython 第3版
初めてのコンピュータサイエンス
訳者まえがき
はじめに
1章	言語処理とPython
1.1 言語の計算処理:テキストと単語
1.1.1 Pythonを使ってみよう
1.1.2 NLTKを使ってみよう
1.1.3 テキストを検索する
1.1.4 語彙を数える
1.2 Pythonをより詳しく:テキストと単語のリスト
1.2.1 リスト
1.2.2 リストの添字を指定する
1.2.3 変数
1.2.4 文字列
1.3 言語の計算処理:簡単な統計処理
1.3.1 頻度分布
1.3.2 よりきめ細かい単語選択
1.3.3 コロケーションとバイグラム
1.3.4 ほかのものを数える
1.4 再びPython:決定を下し処理を制御する
1.4.1 条件式
1.4.2 すべての要素を操作する
1.4.3 入れ子になったコードブロック
1.4.4 ループと条件分岐
1.5 自動自然言語理解
1.5.1 語義曖昧性解消
1.5.2 代名詞解析
1.5.3 言語出力の生成
1.5.4 機械翻訳
1.5.5 音声対話システム
1.5.6 含意関係
1.5.7 NLPの限界
1.6 まとめ
1.7 参考文献
1.8 演習問題
2章	テキストコーパスと語彙資源へのアクセス
2.1 テキストコーパスへのアクセス
2.1.1 グーテンベルクのコーパス
2.1.2 ウェブやチャットのテキスト
2.1.3 ブラウンコーパス
2.1.4 ロイターコーパス
2.1.5 就任演説コーパス
2.1.6 注釈付きのテキストコーパス
2.1.7 ほかの言語のコーパス
2.1.8 テキストコーパスの構造
2.1.9 独自のコーパスを読み込む
2.2 条件付き頻度分布
2.2.1 条件と事象
2.2.2 ジャンルごとに単語を数える
2.2.3 分布の図表作成
2.2.4 バイグラムによるランダムテキストの生成
2.3 Pythonをもっと活用する:コードの再利用
2.3.1 テキストエディタによるプログラムの作成
2.3.2 関数
2.3.3 モジュール
2.4 語彙資源
2.4.1 語彙リストコーパス
2.4.2 発音辞書
2.4.3 比較語彙リスト
2.4.4 2つの語彙目録:ShoeboxとToolbox
2.5 WordNet
2.5.1 語義と同義語
2.5.2 WordNetの階層構造
2.5.3 そのほかの語彙関係
2.5.4 意味類似性
2.6 まとめ
2.7 参考文献
2.8 演習問題
3章	生テキストの処理
3.1 ウェブおよびディスク上のテキストへのアクセス
3.1.1 電子ブック
3.1.2 HTMLの処理
3.1.3 検索エンジンの結果の処理
3.1.4 RSSフィードの処理
3.1.5 ローカルファイルの読み込み
3.1.6 PDFやMSWordなどのバイナリフォーマットからのテキスト抽出
3.1.7 ユーザーからの入力を得る
3.1.8 NLPパイプライン
3.2 文字列:もっとも低いレベルのテキスト処理
3.2.1 基本的な文字列操作
3.2.2 文字列の表示
3.2.3 文字列内の特定の文字へのアクセス
3.2.4 部分文字列へのアクセス
3.2.5 そのほかの文字列操作方法
3.2.6 リストと文字列の違い
3.3 Unicodeを利用したテキストの処理
3.3.1 Unicodeとは何か
3.3.2 エンコードされたテキストをファイルから取り出す
3.3.3 Pythonでのローカルエンコーディングの利用
3.4 単語のパターンを検出するための正規表現
3.4.1 基本的なメタキャラクタの利用
3.4.2 範囲と閉包
3.5 正規表現の有用な応用方法
3.5.1 単語の部分文字列の抽出
3.5.2 単語の部分文字列の処理を進める
3.5.3 語幹を発見する
3.5.4 トークン化されたテキストの検索
3.6 テキストの正規化
3.6.1 ステマー
3.6.2 見出し語化
3.7 テキストのトークン化のための正規表現
3.7.1 シンプルなアプローチによるトークン化
3.7.2 NLTKの正規表現トークナイザ
3.7.3 トークン化のそのほかの問題
3.8 セグメンテーション
3.8.1 文分割
3.8.2 単語分割
3.9 データの整形:リストから文字列へ
3.9.1 リストから文字列への整形
3.9.2 文字列と整形
3.9.3 値を並べる
3.9.4 結果のファイルへの出力
3.9.5 テキストの折り返し
3.10 まとめ
3.11 参考文献
3.12 演習問題
4章	構造化されたプログラムを書く
4.1 基礎へ立ち戻ろう
4.1.1 代入
4.1.2 等価性
4.1.3 条件分岐
4.2 シーケンス
4.2.1 シーケンス型の操作
4.2.2 異なるシーケンス型の結合
4.2.3 ジェネレータ式
4.3 スタイルの問題
4.3.1 Pythonにおけるコーディングスタイル
4.3.2 手続き的スタイルと宣言的スタイル
4.3.3 カウンタの正しい使い方
4.4 関数:構造化プログラミングの基礎
4.4.1 関数の入力と出力
4.4.2 引数を渡す
4.4.3 変数のスコープ
4.4.4 引数の型のチェック
4.4.5 機能を分割する
4.4.6 関数のドキュメントを書く
4.5 関数をもっと活用する
4.5.1 引数としての関数
4.5.2 累積的な関数
4.5.3 高階関数
4.5.4 名前付き引数
4.6 プログラムの開発
4.6.1 Pythonモジュールの構造
4.6.2 複数のモジュールで構成されるプログラム
4.6.3 エラーの原因
4.6.4 デバッグの手法
4.6.5 防衛的プログラミング
4.7 アルゴリズムデザイン
4.7.1 再帰
4.7.2 空間と時間のトレードオフ
4.7.3 動的計画法
4.8 Pythonライブラリの例
4.8.1 Matplotlib
4.8.2 NetworkX
4.8.3 CSV
4.8.4 NumPy
4.8.5 ほかのPythonライブラリ
4.9 まとめ
4.10 参考文献
4.11 演習問題
5章	単語の分類とタグ付け
5.1 タガーの利用
5.2 タグ付きコーパス
5.2.1 タグ付きコーパスの表現
5.2.2 タグ付きコーパスを読み込む
5.2.3 簡易版品詞タグセット
5.2.4 名詞
5.2.5 動詞
5.2.6 形容詞と副詞
5.2.7 非簡易版タグ
5.2.8 タグ付きコーパスの調査
5.3 Pythonのディクショナリを利用して単語を属性にマップする
5.3.1 添字付きリスト対ディクショナリ
5.3.2 Pythonにおけるディクショナリ
5.3.3 ディクショナリを定義する
5.3.4 デフォルトディクショナリ
5.3.5 辞書の値をインクリメントする
5.3.6 複合的なキーと値
5.3.7 辞書を転置する
5.4 自動タグ付け
5.4.1 デフォルトタガー
5.4.2 正規表現タガー
5.4.3 ルックアップタガー
5.4.4 評価
5.5 Nグラムタグ付け
5.5.1 ユニグラムタグ付け
5.5.2 テストデータと訓練データを分離する
5.5.3 一般的なNグラムタグ付け
5.5.4 タガーを組み合わせる
5.5.5 未知語のタグ付け
5.5.6 タガーの保存
5.5.7 性能の限界
5.5.8 文の境界を超えたタグ付け
5.6 変換を利用したタグ付け
5.7 品詞を決定する方法
5.7.1 形態論的な手がかり
5.7.2 統語的な手がかり
5.7.3 意味的な手がかり
5.7.4 新しい単語
5.7.5 品詞タグセットにおける形態論
5.8 まとめ
5.9 参考文献
5.10 演習問題
6章	テキスト分類の学習
6.1 教師あり分類
6.1.1 性別の決定
6.1.2 正しい素性の選択
6.1.3 文書の分類
6.1.4 品詞のタグ付け
6.1.5 文脈の利用
6.1.6 系列分類
6.1.7 系列分類のためのほかの方法
6.2 教師あり分類のそのほかの例
6.2.1 文分割
6.2.2 対話動作のタイプを特定する
6.2.3 含意関係認識
6.2.4 大きなデータセットにスケールアップする
6.3 評価
6.3.1 テストセット
6.3.2 正解率
6.3.3 精度と再現率
6.3.4 混同行列
6.3.5 交差検定
6.4 決定木
6.4.1 エントロピーと情報利得
6.5 単純ベイズ分類器
6.5.1 基本的な確率モデル
6.5.2 ゼロのカウントとスムージング
6.5.3 二値ではない素性
6.5.4 独立の単純さ
6.5.5 二重計算の原因
6.6 最大エントロピー分類器
6.6.1 最大エントロピーモデル
6.6.2 エントロピーの最大化
6.6.3 生成的分類器と条件付き分類器
6.7 言語的パターンのモデル化
6.7.1 モデルから学べることは何か
6.8 まとめ
6.9 参考文献
6.10 演習問題
7章	テキストからの情報抽出
7.1 情報抽出
7.1.1 情報抽出アーキテクチャ
7.2 チャンキング
7.2.1 名詞句チャンキング
7.2.2 タグパターン
7.2.3 正規表現を用いたチャンキング
7.2.4 テキストコーパスの探索
7.2.5 チンキング
7.2.6 チャンクの表現方法:タグと木構造
7.3 チャンカの開発と評価
7.3.1 IOBフォーマットとCoNLL-2000チャンキングコーパスの読み込み
7.3.2 簡単な評価とベースライン
7.3.3 分類器ベースのチャンカの訓練
7.4 言語構造における再帰
7.4.1 カスケードしたチャンカを用いた入れ子構造の構築
7.4.2 木構造
7.4.3 木の走査
7.5 固有表現認識
7.6 関係抽出
7.7 まとめ
7.8 参考文献
7.9 演習問題
8章	文構造の分析
8.1 文法的なジレンマ
8.1.1 言語データと無限の可能性
8.1.2 普遍的な曖昧性
8.2 構文の用途
8.2.1 Nグラムを超えて
8.3 文脈自由文法
8.3.1 簡単な文法
8.3.2 自分自身の文法を定義する
8.3.3 統語構造における再帰
8.4 文脈自由文法を用いた構文解析
8.4.1 再帰下降構文解析
8.4.2 Shift-Reduce構文解析
8.4.3 左隅構文解析器
8.4.4 適格部分文字列表
8.5 依存関係と依存文法
8.5.1 結合価と語彙
8.5.2 文法を拡張する
8.6 文法の開発
8.6.1 Treebankと文法
8.6.2 曖昧性の問題
8.6.3 重み付き文法
8.7 まとめ
8.8 参考文献
8.9 演習問題
9章	素性ベースの文法の構築
9.1 文法素性
9.1.1 構文的一致
9.1.2 属性と制約の利用
9.1.3 用語
9.2 素性構造の処理
9.2.1 包含と単一化
9.3 素性ベース文法の拡張
9.3.1 下位範疇化
9.3.2 主辞の再考
9.3.3 助動詞と転置
9.3.4 非有界依存構造
9.3.5 ドイツ語における格と性
9.4 まとめ
9.5 参考文献
9.6 演習問題
10章	文の意味の解析
10.1 自然言語理解
10.1.1 データベースに対する問い合わせ
10.1.2 自然言語、意味論、論理
10.2 命題論理学
10.3 一階述語論理
10.3.1 構文
10.3.2 一階定理証明
10.3.3 一階述語論理言語のまとめ
10.3.4 モデルにおける真偽値
10.3.5 個体変数と代入
10.3.6 量化子
10.3.7 量化子スコープの曖昧さ
10.3.8 モデルの構築
10.4 英語文の意味論
10.4.1 素性ベース文法の構成意味論
10.4.2 計算
10.4.3 量化子付きNP
10.4.4 他動詞
10.4.5 量化子の再考
10.5 談話意味論
10.5.1 談話表現理論
10.5.2 談話処理
10.6 まとめ
10.7 参考文献
10.8 演習問題
11章	言語データの管理
11.1 コーパスの構造:ケーススタディ
11.1.1 TIMITの構造
11.1.2 注目に値する設計上の特徴
11.1.3 基本的なデータ型
11.2 コーパスのライフサイクル
11.2.1 3つのコーパス構築シナリオ
11.2.2 品質管理
11.2.3 データの管理と発展
11.3 データの取得
11.3.1 ウェブからのデータの取得
11.3.2 ワードプロセッサファイルからのデータの取得
11.3.3 スプレッドシートとデータベースからのデータの取得
11.3.4 データ形式の変換
11.3.5 どの層の注釈を含めるかの決定
11.3.6 標準規格とツール
11.3.7 危機に瀕した言語を処理する際に特に注意すべき事柄
11.4 XMLを利用した処理
11.4.1 言語構造にXMLを利用する
11.4.2 XMLの役割
11.4.3 ElementTreeインターフェイス
11.4.4 ToolboxデータにアクセスするためにElementTreeを使用する
11.4.5 項目の形成
11.5 Toolboxデータを利用する
11.5.1 各項目にフィールドを追加する
11.5.2 Toolboxの語彙を検査する
11.6 OLACメタデータを利用した言語資源の表現
11.6.1 メタデータとは何か
11.6.2 OLAC:オープン言語アーカイブコミュニティ
11.7 まとめ
11.8 参考文献
11.9 演習問題
12章	Pythonによる日本語自然言語処理
Pythonにおける日本語の取り扱い
12.1 日本語コーパスの取り扱い
12.1.1 平文コーパス
12.1.2 タグ付きコーパス
12.1.3 依存構造解析済みコーパス
12.1.4 コーパスを用いたテキスト処理
12.1.5 日本語WordNet
12.1.6 その他の日本語コーパス
12.2 日本語形態素解析
12.2.1 形態素解析アルゴリズム
12.2.2 文字単位分かち書きを使う
12.2.3 MeCabを使う
12.2.4 JUMANを使う
12.2.5 そのほかのトピック
12.3 日本語構文解析
12.3.1 句構造解析
12.3.2 文節チャンキング
12.3.3 CaboChaを使う
12.3.4 KNPを使う
12.3.5 係り受け解析
12.4 日本語意味解析
12.4.1 格フレームとその獲得
12.4.2 日本語LFG
12.4.3 日本語句構造文法(ICOT JPSG)
12.4.4 その他の日本語HPSG
12.4.5 述語項構造解析
12.4.6 照応解析
12.5 さらに学ぶために
12.5.1 ウェブサイト
12.5.2 一般的な教科書
12.5.3 形態素解析
12.5.4 仮名漢字変換
12.5.5 構文解析と意味解析
12.5.6 機械翻訳
12.5.7 情報検索
12.6 演習問題
あとがき:言語への挑戦
参考文献一覧
NLTK索引
索引
ここで紹介する正誤表には、書籍発行後に気づいた誤植や更新された情報を掲載しています。以下のリストに記載の年月は、正誤表を作成し、増刷書籍を印刷した月です。お手持ちの書籍では、すでに修正が施されている場合がありますので、書籍最終ページの奥付でお手持ちの書籍の刷版、刷り年月日をご確認の上、ご利用ください。
P.472 「12.1.2 タグ付きコーパス」
注:この節のコード片を実行するには、12.1.2 第1段落の方法でダウンロードした
JEITAコーパスのzipファイルを同じ場所(通常はnltk_dataディレクトリ)に解凍
する必要がある(これは、p.476の1行目のコードについても同じである)。
もしくは、p.473上のコード片の2行目を、
from nltk.corpus.util import LazyCorpusLoader
jeita = LazyCorpusLoader(&apos;jeita&apos;, ChasenCorpusReader, r&apos;.*chasen&apos;, 
encoding=&apos;utf-8&apos;)
としてコーパスリーダーを作成すれば、ダウンロードしたzipファイルをそのまま
読み込むことができる。
p.473 真ん中の段落
誤:それ自体がタプルになっており
正:タブ区切りの文字列になっており
p.473 真ん中のコード片1行目
誤:print &apos;\nEOS\n&apos;.join([&apos;\n&apos;.join("%s/%s" % (w[0],w[1][2]) for w in sent) 
for sent in jeita.tagged_sents()[2170:2173]])
正:print &apos;\nEOS\n&apos;.join([&apos;\n&apos;.join("%s/%s" % (w[0],w[1].split(&apos;\t&apos;)[2]) 
for w in sent) for sent in jeita.tagged_sents()[2170:2173]])
p.474 「12.1.3 依存構造解析済みコーパス」
注:このコード片を実行するには、12.1.3 第1段落の方法でダウンロードした
KNBコーパスのzipファイルを同じ場所(通常はnltk_dataディレクトリ)に解凍する
必要がある。もしくは、p.474で述べたfileidを列挙する代わりに、
knbc = LazyCorpusLoader(&apos;knbc/corpus1&apos;, KNBCorpusReader, r&apos;.*/KN.*&apos;, 
encoding=&apos;euc-jp&apos;)
としてコーパスリーダーを作成すれば、ダウンロードしたzipファイルを
そのまま読み込むことができる。ただし、この場合コーパス中のファイルの
順番の同一性は保証されない。
p.474下のコード片1行目
誤:>>> print &apos;\n&apos;.join( &apos;&apos;.join(sent) for sent in knbc.words() )
正: >>> print &apos;&apos;.join( knbc.words()[:100] )
p.475真ん中のコード片1行目
誤:>>> print &apos;\n&apos;.join( &apos; &apos;.join("%s/%s"%(w[0], w[1][2]) for w in sent) 
for sent in knbc.tagged_words()[0:20] )
正:>>> print &apos;\n&apos;.join( &apos; &apos;.join("%s/%s"%(w[0], w[1].split(&apos; &apos;)[2]) 
for w in sent) for sent in knbc.tagged_sents()[0:20] )
p.476 コード中
誤:>>> print &apos; &apos;.join( set(w for w,t in genpaku.tagged_words() if 
t[0] == u"コウショウ") )
正:>>> print &apos; &apos;.join( set(w for w,t in genpaku.tagged_words() if 
t.split(&apos;\t&apos;)[0] == u"コウショウ") )
p.481 4行目
誤:「12.5 参考文献」
正:「12.5 さらに学ぶために」
p.494 第4段落
誤:JUMANを元にして
正:ChaSenを元にして
p.522 5行目
誤:言語処理学辞典
正:言語処理学事典
p.539 参考文献に下記を追加
[清瀬1989] 清瀬義三郎則府『日本語文法新論 派生文法序説(桜楓社、1989年)
P.540 参考文献に下記を追加
[鈴木1972] 鈴木重幸『日本語文法・形態論』(むぎ書房、1972年)
ここで紹介する正誤表には、書籍発行後に気づいた誤植や更新された情報を掲載しています。以下のリストに記載の年月は、正誤表を作成し、増刷書籍を印刷した月です。お手持ちの書籍では、すでに修正が施されている場合がありますので、書籍最終ページの奥付でお手持ちの書籍の刷版、刷り年月日をご確認の上、ご利用ください。
P482 最初のコード編の末尾にコードを追加
誤:[u"まつ", {&apos;pos&apos;:&apos;V-T&apos;, &apos;lemma&apos;:u"待つ"}],
[u"つ", {&apos;pos&apos;:&apos;N&apos;, &apos;lemma&apos;:u"津"}]
]
正:
[u"まつ", {&apos;pos&apos;:&apos;V-T&apos;, &apos;lemma&apos;:u"待つ"}],
[u"つ", {&apos;pos&apos;:&apos;N&apos;, &apos;lemma&apos;:u"津"}]
]
matrie = nltk.defaultdict(dict)
_BOS_ENTRY = {&apos;length&apos;:1, &apos;pos&apos;:&apos;BOS&apos;, &apos;lemma&apos;: u&apos;BOS&apos;, &apos;cost&apos;: 0}
_EOS_ENTRY = {&apos;length&apos;:1, &apos;pos&apos;:&apos;EOS&apos;, &apos;lemma&apos;: u&apos;EOS&apos;, &apos;cost&apos;: 0}
P552 項目「素性」(そせい)
この項目をP553へ移す
O'Reilly Japan - 入門 自然言語処理
